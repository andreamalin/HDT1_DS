{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Para PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "\n",
    "# Reglas\n",
    "from apyori import apriori\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "seed = random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable classification\n",
    "train_data = pd.read_csv('./data/train.csv', encoding = \"ISO-8859-1\")\n",
    "test_data = pd.read_csv('./data/test.csv', encoding = \"ISO-8859-1\")\n",
    "variables = pd.read_csv('./data/variables.txt', encoding = \"ISO-8859-1\")\n",
    "quant_vars = list(variables.loc[(variables['Clasification'] == 'Cuantitativa')]['Variable'].values)\n",
    "quali_vars = list(variables.loc[(variables['Clasification'] == 'Cualitativa')]['Variable'].values)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando las variables numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[quant_vars].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quant_vars:\n",
    "    data = train_data[var].dropna(how='all', axis=0)\n",
    "    \n",
    "    # Gráfico\n",
    "    sns.displot(data, kde=True)\n",
    "\n",
    "    # Mostrando normalidad\n",
    "    print('\\033[1m' + var + '\\033[0m' + ': Kurtosis:', stats.kurtosis(data), 'Skewness:', stats.skew(data), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando las variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quali_vars:\n",
    "  plt.figure(figsize=(20,5))\n",
    "  train_data[var].value_counts().plot(kind='bar')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "corrmat = train_data.corr()\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obteniendo la relacion entre las variables mas significativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data[cols],hue=\"SalePrice\")\n",
    "plt.show()\n",
    "# quant_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_train_data = train_data.copy()\n",
    "copied_train_data = copied_train_data.fillna(0)\n",
    "numeric = copied_train_data[quant_vars].copy()\n",
    "numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_cuadrado, p_valor = calculate_bartlett_sphericity(numeric)\n",
    "chi_cuadrado, p_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo,kmo_modelo = calculate_kmo(numeric)\n",
    "kmo_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El índice es mayor a 0.5, por lo que la adecuación muestral para un análisis factorial es aceptable. Probablemente esto nos indique que voy a tener buenos resultados haciendo el PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe = make_pipeline(StandardScaler(), PCA()) #Se escalan los datos y luego se le aplica PCA\n",
    "pca_pipe.fit(numeric)\n",
    "#Se extrae el modelo del pipeline\n",
    "modelo_pca = pca_pipe.named_steps['pca']\n",
    "\n",
    "index = []\n",
    "for i in range(len(numeric.columns)):\n",
    "    index.append('PC' + str(i + 1))\n",
    "\n",
    "pd.DataFrame(\n",
    "    data = modelo_pca.components_,\n",
    "    columns = numeric.columns,\n",
    "    index = index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "componentes = modelo_pca.components_\n",
    "plt.imshow(componentes.T)\n",
    "plt.yticks(range(len(numeric.columns)), numeric.columns)\n",
    "plt.xticks(range(len(numeric.columns)), np.arange(modelo_pca.n_components_)+1)\n",
    "plt.grid(False)\n",
    "fig.set_size_inches(19, 11, forward=True)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------')\n",
    "print('Porcentaje de varianza explicada por cada componente')\n",
    "print('----------------------------------------------------')\n",
    "print(modelo_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "ax.bar(\n",
    "    x      = np.arange(modelo_pca.n_components_) + 1,\n",
    "    height = modelo_pca.explained_variance_ratio_\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(len(numeric.columns)) + 1, modelo_pca.explained_variance_ratio_):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x,y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "ax.set_xticks(np.arange(modelo_pca.n_components_) + 1)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_title('Porcentaje de varianza explicada por cada componente')\n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Por. varianza explicada')\n",
    "fig.set_size_inches(15, 3, forward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porcentaje de varianza explicada acumulada\n",
    "# ==============================================================================\n",
    "prop_varianza_acum = modelo_pca.explained_variance_ratio_.cumsum()\n",
    "print('------------------------------------------')\n",
    "print('Porcentaje de varianza explicada acumulada')\n",
    "print('------------------------------------------')\n",
    "print(prop_varianza_acum)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "ax.plot(\n",
    "    np.arange(len(numeric.columns)) + 1,\n",
    "    prop_varianza_acum,\n",
    "    marker = 'o'\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(len(numeric.columns)) + 1, prop_varianza_acum):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x,y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha='center'\n",
    "    )\n",
    "    \n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_xticks(np.arange(modelo_pca.n_components_) + 1)\n",
    "ax.set_title('Porcentaje de varianza explicada acumulada')\n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Por. varianza acumulada')\n",
    "fig.set_size_inches(15, 5, forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento Reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage = 'GarageYrBlt'\n",
    "\n",
    "copied_data = train_data.copy()\n",
    "copied_data[garage] = copied_data[garage].fillna(0)\n",
    "categories = copied_data[quali_vars].astype(object).copy()\n",
    "\n",
    "categories[garage] = categories[garage].astype(np.int16)\n",
    "categories[garage] = categories[garage].astype('string')\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quali_vars:\n",
    "  print(categories[var].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_row, shape_column = categories.shape\n",
    "# Se van a convertir los datos a una lista de listas\n",
    "records = []\n",
    "\n",
    "for i in range(0, 32):\n",
    "    records.append([str(categories.values[i,j]) for j in range(0, 5)]) \n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mínimo de cobertura o soporte es 20% y el mínimo de confianza es 70%\n",
    "reglas_asociacion = apriori(records,min_support=0.2, min_confidence = 0.8)\n",
    "reglas = list(reglas_asociacion)\n",
    "len(reglas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reglas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(reglas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a8e74bc410295dd7b3e2a92a04bda485b935e7c35103812674b9cdd1b25ea1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
